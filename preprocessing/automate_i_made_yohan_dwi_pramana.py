# -*- coding: utf-8 -*-
"""automate_I_Made_Yohan_Dwi Pramana.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yNGQh9Nnnrhy0yvHS5UrAxN8Dfh-fem8
"""

import pandas as pd
import re
import string
import os
import nltk
from nltk.corpus import stopwords


nltk.download('stopwords', quiet=True)

STOP_ID = set(stopwords.words('indonesian'))
STOP_EN = set(stopwords.words('english'))
STOP_CUSTOM = {'iya', 'yaa', 'loh', 'sih', 'nya', 'ga', 'ya'}
STOPWORDS_ALL = STOP_ID | STOP_EN | STOP_CUSTOM


def cleaning_text(text):
    text = re.sub(r'@[A-Za-z0-9_]+', '', text)
    text = re.sub(r'#[A-Za-z0-9_]+', '', text)
    text = re.sub(r'RT\s+', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'\d+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def casefolding_text(text):
    return text.lower()


def normalize_slang(text):
    return ' '.join(slangwords.get(w, w) for w in text.split())


def filtering_text(words):
    return [w for w in words if w not in STOPWORDS_ALL]


def preprocess_text(text):
    text = cleaning_text(str(text))
    text = casefolding_text(text)
    text = normalize_slang(text)
    tokens = text.split()          # lebih cepat dari word_tokenize
    tokens = filtering_text(tokens)
    return ' '.join(tokens)


def preprocess_dataframe(df, text_column='content'):
    df = df.copy()
    df['text_final'] = df[text_column].apply(preprocess_text)
    return df


def preprocess_csv(input_path, output_path):
    if not os.path.exists(input_path):
        raise FileNotFoundError("File input tidak ditemukan")

    df = pd.read_csv(input_path)
    df_clean = preprocess_dataframe(df)
    df_clean.to_csv(output_path, index=False)
    return df_clean


if __name__ == "__main__":
    preprocess_csv(
        input_path='Kredivo.csv',
        output_path='preprocessing/preprocessed_kredivo.csv'
    )
